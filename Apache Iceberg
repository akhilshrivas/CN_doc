What is Apache Iceberg?

Apache Iceberg is an open table format for large-scale analytic datasets — like what you’d store in data lakes (e.g., on S3, HDFS, or Azure Blob).
It brings database-like features (ACID transactions, schema evolution, partition management) to data files stored in open formats such as Parquet, Avro, or ORC.

Think of Iceberg as a “table layer” on top of your raw data files, making them easier, faster, and safer to query using engines like Spark, Flink, Trino, Hive, or Dremio.

Why Iceberg?

Traditional data lakes have some problems:

No ACID transactions (simultaneous writes can corrupt data)

Schema changes are hard to manage

Partitioning logic is static and error-prone

Metadata grows fast (slowing queries)

Iceberg fixes this by maintaining a metadata tree (called manifests and snapshots) that keeps track of what data files make up a table and how they evolve over time.

Key Features
Feature	Description
ACID transactions	Safe concurrent reads/writes
Schema evolution	Add, rename, or drop columns without breaking existing queries
Hidden partitioning	Query engines don’t need to know partition columns
Time travel	Query historical versions of the table
Snapshot isolation	Reads are consistent and isolated from writes
Compatible with multiple engines	Works with Spark, Flink, Trino, Hive, and more
Architecture Overview

An Iceberg table has three layers:

Data files – stored in Parquet/Avro/ORC format

Metadata files – track what data files exist and their schema

Manifest lists – link snapshots and versions of the table

Each change (insert, delete, update) creates a new snapshot of the table.

Simple Example

Let’s say we have a data lake on Amazon S3, and we use Apache Spark to manage an Iceberg table.

Step 1: Create an Iceberg Table
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("IcebergExample") \
    .config("spark.sql.catalog.local", "org.apache.iceberg.spark.SparkCatalog") \
    .config("spark.sql.catalog.local.type", "hadoop") \
    .config("spark.sql.catalog.local.warehouse", "s3://my-bucket/warehouse/") \
    .getOrCreate()

# Create a table
spark.sql("""
CREATE TABLE local.db.customers (
    id BIGINT,
    name STRING,
    city STRING,
    age INT
) USING iceberg
""")

Step 2: Insert Data
spark.sql("""
INSERT INTO local.db.customers VALUES
(1, 'Akhil', 'Bhopal', 22),
(2, 'Ravi', 'Indore', 25)
""")


This creates a new snapshot of the table with two records.

Step 3: Query the Table
spark.sql("SELECT * FROM local.db.customers").show()


Output:

+---+------+--------+---+
|id |name  |city    |age|
+---+------+--------+---+
| 1 |Akhil |Bhopal  |22 |
| 2 |Ravi  |Indore  |25 |
+---+------+--------+---+

Step 4: Time Travel

You can query an older snapshot of the table:

spark.sql("""
SELECT * FROM local.db.customers
VERSION AS OF 1
""")


This lets you see the data as it was before updates or deletions — useful for auditing and debugging.

Real-World Use Case

Netflix, Adobe, and LinkedIn use Iceberg at scale.

Common scenarios:

Data lakes where analysts need consistent, queryable data.

Slowly changing data (you want updates without rewriting entire partitions).

Multi-engine environments (Spark + Trino + Flink all reading same table).

Summary
Aspect	Description
Type	Open table format for data lakes
Storage	Works with Parquet, Avro, ORC
Supports	Spark, Flink, Trino, Hive
Key Advantage	ACID transactions + schema evolution + time travel
Best For	Large, multi-engine, cloud-based analytics systems
